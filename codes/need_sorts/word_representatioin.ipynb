{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_representatioin.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDU0n9xQC6AFrrA2+M+7W4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanghunOh/learn_NaturalLanguageProcessing/blob/main/codes/word_representatioin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기본 토큰화 "
      ],
      "metadata": {
        "id": "GdnRpPCg_sgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7IMQl7Fj_sDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL6oYtpp7HEC",
        "outputId": "598d9728-e9cf-4af3-e973-a8101fbba1dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I'm, we're, rock 'n' roll\"\n",
        "sentence = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
        "sentence = \"m.p.h, Ph.D나 AT&T\"\n",
        "word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZCej-e7HjF",
        "outputId": "6605e56b-aafd-49fc-b154-2abdc12e1ed0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['m.p.h', ',', 'Ph.D나', 'AT', '&', 'T']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = \"I am actively looking for Ph.D. students. and you are a Ph.D student\"\n",
        "sentence = \"IP 231.68.56.31 에 들어가서 로그 파일 저장해 otter@gmail.com로 보내줘\"\n",
        "sentence = \"I'm actively looking for Ph.D. students, I get the same question.\"\n",
        "sent_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7Yo6Fjz7LQK",
        "outputId": "e488e4ea-1434-4808-a124-b3085c620ddb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm actively looking for Ph.D. students, I get the same question.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한글 토큰화 and 품사 달기"
      ],
      "metadata": {
        "id": "96baVu97_zFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk : PRP(인칭 대명사), VBP(동사), RB(부사), VBG(현재부사), IN(전치사), NNP(고유 명사), NNS(복수형 명사), CC(접속사), DT(관사)"
      ],
      "metadata": {
        "id": "EVazg69dKeAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text=\"열심히 코딩한 당신, 연휴에는 여행을 가봐요.\"\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "x=word_tokenize(text)\n",
        "print(pos_tag(x))\n",
        "# [('열심히', 'JJ'), ('코딩한', 'NNP'), ('당신', 'NNP'), (',', ','), ('연휴에는', 'NNP'), ('여행을', 'NNP'), ('가봐요', 'NNP'), ('.', '.')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wkyfBGA7T8M",
        "outputId": "aa34764a-fe97-42c0-d273-6e82a1caa03b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('열심히', 'JJ'), ('코딩한', 'NNP'), ('당신', 'NNP'), (',', ','), ('연휴에는', 'NNP'), ('여행을', 'NNP'), ('가봐요', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install konlpy \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiWU_VMC_1Zj",
        "outputId": "90e09e28-ea97-4d5f-8e19-bc558929d569"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt();\t\t\tprint(okt.morphs(text))\n",
        "# ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']  \n",
        "print(okt.pos(text))  \n",
        "# [('열심히','Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]  \n",
        "print(okt.nouns(text))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00BxfEJFAcOp",
        "outputId": "30561115-3946-44a2-acba-29b2ecb83f75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요', '.']\n",
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb'), ('.', 'Punctuation')]\n",
            "['코딩', '당신', '연휴', '여행']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##word onehotencoding with tensorflow"
      ],
      "metadata": {
        "id": "IB-QnxqmKr9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "text=\"댓글은 인터넷 게시물 밑에 남길 수 있는 짧은 글이다.\"\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text]);\t\ttokenizer.word_index;\t\t\n",
        "# {'게시물': 3, '글이다': 9, '남길': 5, '댓글은': 1, '밑에': 4, '수': 6, '인터넷': 2, '있는': 7, '짧은': 8}\n",
        "sub_text=\"유즈넷(USENET) 시절 'reply'를 '리플라이', '답장' 등으로 사용하였다\"\t\n",
        "tokenizer.texts_to_sequences([sub_text])[0]\n",
        "# []\t\t\t\t\t\t\t\t\t# 사전 있는 것만 확인 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_T9cASSAdCg",
        "outputId": "cbf9ac76-3c82-484b-94a4-4c80be9509a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_text=\"보통 인터넷 게시물 밑에는 댓글란이 있어 그 게시물과 관련하여 독자는 의견을 표할 수 있다\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "# [2, 3, 6]\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.keras.utils.to_categorical(encoded))\n",
        "# [[0. 0. 1. 0. 0. 0. 0.]\n",
        "#  [0. 0. 0. 1. 0. 0. 0.]\n",
        "#  [0. 0. 0. 0. 0. 0. 1.]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GreqT67ZLelO",
        "outputId": "15848dde-49be-446c-c647-6af2787cfa61"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##원형복원"
      ],
      "metadata": {
        "id": "py_rZnoeS5kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        ">>> n=WordNetLemmatizer()\n",
        ">>> words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        ">>> print([n.lemmatize(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te42OZMfLuDM",
        "outputId": "00c5ff66-4149-4613-b67c-b796ae50bcf9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from nltk.stem import PorterStemmer\n",
        ">>> s=PorterStemmer()\n",
        ">>> words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        ">>> print([s.stem(w) for w in words])\n",
        "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzMhtYKsTCLK",
        "outputId": "b2b8bf09-9718-4b2e-fbd3-7d14645c4fed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['polici',\n",
              " 'do',\n",
              " 'organ',\n",
              " 'have',\n",
              " 'go',\n",
              " 'love',\n",
              " 'live',\n",
              " 'fli',\n",
              " 'die',\n",
              " 'watch',\n",
              " 'ha',\n",
              " 'start']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OtEaXrsNTMRK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}